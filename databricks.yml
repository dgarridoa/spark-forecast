bundle:
  name: demand-forecast

artifacts:
  wheel:
    type: whl
    path: ./
    build: poetry build

workspace:
  auth_type: pat

resources:
  experiments:
    mlflow-demand-forecast:
      name: ${workspace.root_path}/artifacts/${bundle.target}-${bundle.name}
  jobs:
    demand-forecast:
      name: ${bundle.target}-demand-forecast
      job_clusters:
        - job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          new_cluster:
            spark_version: "13.3.x-cpu-ml-scala2.12"
            spark_conf:
              spark.databricks.sql.initial.catalog.name: "demand-forecast"
            spark_env_vars:
              MLFLOW_EXPERIMENT_NAME: "${resources.experiments.mlflow-demand-forecast.name}"
              WORKSPACE_FILE_PATH: "/Workspace${workspace.root_path}/files"
            num_workers: 1
            node_type_id: "Standard_DS3_v2"
      tasks:
        - task_key: "create_database"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "create_database"
            parameters:
              - "--conf-file"
              - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "ingest"
          depends_on:
            - task_key: "create_database"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
             package_name: "spark_forecast"
             entry_point: "ingest"
             parameters:
             - "--conf-file"
             - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "split"
          depends_on:
            - task_key: "ingest"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
             package_name: "spark_forecast"
             entry_point: "split"
             parameters:
             - "--conf-file"
             - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "exponential_smoothing"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "ExponentialSmoothing"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "autoarima"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "AutoARIMA"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "prophet"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "Prophet"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "xgboost"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "XGBModel"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "random_forest"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "RandomForest"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "croston"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "Croston"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "mean"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "NaiveMean"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "moving_average"
          depends_on:
            - task_key: "split"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "model"
            parameters:
            - "--model-name"
            - "NaiveMovingAverage"
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
        - task_key: "evaluation"
          depends_on:
            - task_key: "exponential_smoothing"
            - task_key: "autoarima"
            - task_key: "prophet"
            - task_key: "xgboost"
            - task_key: "random_forest"
            - task_key: "croston"
            - task_key: "mean"
            - task_key: "moving_average"
          job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
          python_wheel_task:
            package_name: "spark_forecast"
            entry_point: "evaluation"
            parameters:
            - "--conf-file"
            - "/Workspace${workspace.root_path}/files/conf/${bundle.target}_config.yml"
          libraries:
            - whl: ./dist/*.whl
targets:
  dev:
    mode: development
    default: true
  staging:
    mode: production
    workspace:
      root_path: /Shared/.bundle/${bundle.name}/${bundle.target}
    run_as:
      user_name: diego.garrido.6568@gmail.com
  prod:
    mode: production
    workspace:
      root_path: /Shared/.bundle/${bundle.name}/${bundle.target}
    run_as:
      user_name: diego.garrido.6568@gmail.com
    resources:
      jobs:
        demand-forecast:
          job_clusters:
            - job_cluster_key: "${bundle.target}-${bundle.name}-job-cluster"
              new_cluster:
                driver_node_type_id: "Standard_DS3_v2"
                node_type_id: "Standard_F64s_v2"
          email_notifications:
            on_start: ["diego.garrido.6568@gmail.com"]
            on_success: ["diego.garrido.6568@gmail.com"]
            on_failure: ["diego.garrido.6568@gmail.com"]
          schedule:
            quartz_cron_expression: "16 59 23 ? * Mon"
            timezone_id: "America/Santiago"
            pause_status: "PAUSED"
